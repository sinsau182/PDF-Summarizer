import { CreateChatCompletionResponse, CreateChatCompletionStreamResponse } from "../generated/textGen";
import { Client } from "./client";
import { TEXT_MODELS } from "./constants";
import { Stream } from "./stream";
/**
 * Chat completion API.  Expected to be used through `client.chat.completions`.
 */
export declare class Chat {
    readonly completions: Completions;
    constructor(client: Client);
    /**
     * Returns a list of models available to be used in the API.
     * {@link TEXT_MODELS}
     */
    listAllModels(): readonly ["llama-2-13b-chat", "llama-2-70b-chat", "codellama-7b-instruct", "codellama-13b-instruct", "codellama-34b-instruct", "codellama-70b-instruct", "mistral-7b-instruct", "mixtral-8x7b-instruct", "nous-hermes-2-mixtral-8x7b-dpo", "nous-hermes-2-mistral-7b-dpo"];
}
/**
 * Interface for create chat completion request message.
 *
 * @property content - the contents of the message. `content` is required for all
 * messages.
 * @property  role - {@link ChatCompletionRole} The role of the messages author.
 * One of `system`, `user`, or `assistant`.
 */
export interface ChatCompletionRequestMessage {
    content: string | null;
    role: ChatCompletionRole;
}
/**
 * Interface for create chat completion response_format field.
 *
 * @property type - the type of format.  Currently only `json_object` is supported.
 * @property schema - the schema of the response format.  This is a JSON object
 */
export interface ChatCompletionRequestResponseFormat {
    type: "json_object";
    schema: Record<string, any>;
}
/**
 * ChatCompletionCreateRequest class.  Can be used as an interface, but also
 * instantiates the class handling stream default behavior with stream default
 * being disabled if not explicitly set to on.
 *
 * @property messages - messages - An array of {@link ChatCompletionRequestMessage}
 * comprising the conversation so far.
 * @property  model - ID of the model to use.
 * @property  frequency_penalty - Number between -2.0 and 2.0. Positive values
 * penalize new tokens based on their existing frequency in the text so far,
 * decreasing the model's likelihood to repeat the same line verbatim.
 * @property  max_tokens - The maximum number of tokens to generate in the chat
 * completion. The total length of input tokens and generated tokens is limited
 * by the model's context length.
 * @property  presence_penalty - Number between -2.0 and 2.0. Positive values
 * penalize new tokens based on whether they appear in the text so far,
 * increasing the model's likelihood to talk about new topics.
 * @property  stop - up to 4 sequences where
 * the API will stop generating further tokens.
 * @property  stream - If set, partial message deltas will be sent. Tokens will be
 * sent as data-only.  Defaults to false.
 * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
 * as they become available, with the stream terminated by a `data: [DONE]`
 * message.  This output is processed through {@link Stream} and returns
 * {@link CreateChatCompletionStreamResponse} if set to `true`.  If set to
 * `false` or not specified, returns {@link CreateChatCompletionResponse}
 * @property  temperature - What sampling temperature to use, between 0 and 2.
 * Higher values like 0.8 will make the output more random, while lower values
 * like 0.2 will make it more focused and deterministic.
 * We generally recommend altering this or `top_p` but not both.
 * @property  top_p - An alternative to sampling with temperature, called nucleus
 * sampling, where the model considers the results of the tokens with top_p
 * probability mass. So 0.1 means only the tokens comprising the top 10%
 * probability mass are considered.
 * We generally recommend altering this or `temperature` but not both.
 */
export declare class ChatCompletionCreateRequest {
    messages: ChatCompletionRequestMessage[];
    model: TextModel;
    frequency_penalty?: number;
    max_tokens?: number;
    presence_penalty?: number;
    stop?: string | Array<string>;
    stream?: boolean;
    temperature?: number;
    top_p?: number;
    response_format?: ChatCompletionRequestResponseFormat;
    /**
     * Constructor for the ChatCompletionCreateRequest.
     *
     * @param request - can contain the following fields, but must contain messages
     * and model.
     */
    constructor(request: Record<string, any>);
}
export interface ChatCompletionCreateRequestNonStream extends ChatCompletionCreateRequest {
    stream?: false | undefined;
}
export interface ChatCompletionCreateRequestStream extends ChatCompletionCreateRequest {
    stream?: true;
}
export type ChatCompletionRole = "system" | "user" | "assistant";
export type TextModel = (typeof TEXT_MODELS)[number];
/**
 * Chat completions API.
 *
 * @param client - Uses the {@link Client} class to manage requests.
 */
export declare class Completions {
    readonly client: Client;
    constructor(client: Client);
    create(request: ChatCompletionCreateRequestNonStream, endpoint?: string): Promise<CreateChatCompletionResponse>;
    create(request: ChatCompletionCreateRequestStream, endpoint?: string): Promise<Stream<CreateChatCompletionStreamResponse>>;
    create(body: ChatCompletionCreateRequest): Promise<Stream<CreateChatCompletionStreamResponse> | CreateChatCompletionResponse>;
}
