"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.Completions = exports.ChatCompletionCreateRequest = exports.Chat = void 0;
const constants_1 = require("./constants");
const error_1 = require("./error");
const stream_1 = require("./stream");
/**
 * Chat completion API.  Expected to be used through `client.chat.completions`.
 */
class Chat {
    constructor(client) {
        this.completions = new Completions(client);
    }
    /**
     * Returns a list of models available to be used in the API.
     * {@link TEXT_MODELS}
     */
    listAllModels() {
        return constants_1.TEXT_MODELS;
    }
}
exports.Chat = Chat;
// The codegen class is dependent on an enum, which we can't easily convert to
// a string, so instead we've recreated this class to do basic type checking
// of requests, validation, param requirements, though this is dependent on some
// codegen classes as well.  CreateChatCompletionRequestMessage and TextModel
// are not codegen.
/**
 * ChatCompletionCreateRequest class.  Can be used as an interface, but also
 * instantiates the class handling stream default behavior with stream default
 * being disabled if not explicitly set to on.
 *
 * @property messages - messages - An array of {@link ChatCompletionRequestMessage}
 * comprising the conversation so far.
 * @property  model - ID of the model to use.
 * @property  frequency_penalty - Number between -2.0 and 2.0. Positive values
 * penalize new tokens based on their existing frequency in the text so far,
 * decreasing the model's likelihood to repeat the same line verbatim.
 * @property  max_tokens - The maximum number of tokens to generate in the chat
 * completion. The total length of input tokens and generated tokens is limited
 * by the model's context length.
 * @property  presence_penalty - Number between -2.0 and 2.0. Positive values
 * penalize new tokens based on whether they appear in the text so far,
 * increasing the model's likelihood to talk about new topics.
 * @property  stop - up to 4 sequences where
 * the API will stop generating further tokens.
 * @property  stream - If set, partial message deltas will be sent. Tokens will be
 * sent as data-only.  Defaults to false.
 * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
 * as they become available, with the stream terminated by a `data: [DONE]`
 * message.  This output is processed through {@link Stream} and returns
 * {@link CreateChatCompletionStreamResponse} if set to `true`.  If set to
 * `false` or not specified, returns {@link CreateChatCompletionResponse}
 * @property  temperature - What sampling temperature to use, between 0 and 2.
 * Higher values like 0.8 will make the output more random, while lower values
 * like 0.2 will make it more focused and deterministic.
 * We generally recommend altering this or `top_p` but not both.
 * @property  top_p - An alternative to sampling with temperature, called nucleus
 * sampling, where the model considers the results of the tokens with top_p
 * probability mass. So 0.1 means only the tokens comprising the top 10%
 * probability mass are considered.
 * We generally recommend altering this or `temperature` but not both.
 */
class ChatCompletionCreateRequest {
    /**
     * Constructor for the ChatCompletionCreateRequest.
     *
     * @param request - can contain the following fields, but must contain messages
     * and model.
     */
    constructor(request) {
        this.messages = request.messages;
        this.model = request.model;
        for (const key in request) {
            const value = request[key];
            if (value === undefined || value === null) {
                delete request[key];
            }
        }
        Object.assign(this, request);
        if (this.stream === undefined) {
            this.stream = false;
        }
    }
}
exports.ChatCompletionCreateRequest = ChatCompletionCreateRequest;
/**
 * Chat completions API.
 *
 * @param client - Uses the {@link Client} class to manage requests.
 */
class Completions {
    constructor(client) {
        this.client = client;
    }
    /**
     * Create a chat completion, either streaming or non-streaming.
     *
     * @param request - {@link ChatCompletionCreateRequest} including the
     * required fields of messages and model.  Defaults to non-streaming.
     * @param endpoint - used to direct request to a different endpoint.  Defaults
     * to `https://text.octoai.run/v1/chat/completions` if no endpoint is provided
     * or uses the SecureLink API if set on client instantiation.
     */
    async create(request, endpoint) {
        // Stalls throwing an error until API used due to importing it in client.
        if (this.client.headers.Authorization === "") {
            (0, error_1.throwOctoAITokenRequiredError)();
        }
        if (!endpoint) {
            endpoint = `${this.client.secureLink ? constants_1.SECURELINK_TEXT_GEN_API : constants_1.DEFAULT_TEXT_GEN_API}/chat/completions`;
        }
        const processedRequest = new ChatCompletionCreateRequest(request);
        if (request.stream) {
            const response = await this.client.inferStream(endpoint, processedRequest);
            return stream_1.Stream.fromSSEResponse(response, new AbortController());
        }
        return this.client.infer(endpoint, processedRequest);
    }
}
exports.Completions = Completions;
//# sourceMappingURL=data:application/json;base64,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