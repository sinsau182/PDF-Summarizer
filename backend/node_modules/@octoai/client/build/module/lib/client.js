"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.Client = void 0;
const version_1 = require("../version");
const asset_1 = require("./asset");
const chat_1 = require("./chat");
const completions_1 = require("./completions");
const error_1 = require("./error");
const tune_1 = require("./tune");
/**
 * A client that allows inferences from existing OctoAI endpoints.  Sets
 * various headers, establishes clients for {@link Chat} under `Client.chat`,
 * {@link AssetLibrary} under `Client.asset`, {@link FineTuningClient} under
 * `Client.tune`, and will check for `OCTOAI_TOKEN`
 * from environment variable if no token is provided.
 *
 * @throws {@link OctoAIClientError} - For client-side failures (throttled, no token)
 * @throws {@link OctoAIServerError} - For server-side failures (unreachable, etc)
 *
 * @remarks
 * You can create an OctoAI API token by following the guide at
 * {@link https://docs.octoai.cloud/docs/how-to-create-an-octoai-access-token |
 * How to Create an OctoAI Access Token}
 */
class Client {
    /**
     * Constructor for the Client class.
     *
     * @param token - OctoAI token.  If none is set, checks for an `OCTOAI_TOKEN`
     * envvar, or will default to null.
     * @param secureLink - Set to true to use SecureLink API instead of public API
     */
    constructor(token, secureLink = false) {
        token = token || process.env.OCTOAI_TOKEN || null;
        if (!token) {
            console.warn("OCTOAI_TOKEN is not set as an environment variable and " +
                "wasn't passed to the Client constructor as a token." +
                " You will only be able to use QuickStart OctoAI endpoints.  " +
                "client.asset, client.chat, and client.tune will throw errors if " +
                "accessed.");
        }
        const headers = {
            "Content-Type": "application/json",
            "User-Agent": `octoai-ts-${version_1.LIB_VERSION}`,
            Authorization: "",
            "X-OctoAI-Async": "", // 0 disabled, 1 enabled
            Accept: "",
        };
        if (token) {
            headers.Authorization = `Bearer ${token}`;
        }
        this.secureLink = secureLink;
        this.headers = headers;
        this.chat = new chat_1.Chat(this);
        this.asset = new asset_1.AssetLibrary(this);
        this.tune = new tune_1.FineTuningClient(this);
        this.completions = new completions_1.CompletionsAPI(this);
    }
    /**
     * Send a request to the given endpoint with inputs as request body.
     * For LLaMA2 LLMs, this requires `"stream": false` in the inputs.  To stream
     * for LLMs, please see the {@link inferStream} method.
     *
     * @param endpointUrl - Target URL to run inference
     * @param inputs - Necessary inputs for the endpointURL to run inference
     *
     * @returns JSON outputs from the endpoint
     */
    async infer(endpointUrl, inputs) {
        const response = await fetch(endpointUrl, {
            method: "POST",
            headers: this.headers,
            body: JSON.stringify(inputs),
        });
        if (!response.ok) {
            await (0, error_1.throwOctoAIError)(response);
        }
        return response.json();
    }
    /**
     * Stream text event response body for supporting endpoints.  This is an
     * alternative to loading all response body into memory at once.  Recommended
     * for use with LLM models.  Requires `"stream": true` in the inputs for
     * LLaMA2 LLMs.
     *
     * @param endpointUrl - Target URL to run inference
     * @param inputs - Necessary inputs for the endpointURL to run inference
     * @returns Compatible with getReader method.
     *
     * @remarks
     * This allows you to stream back tokens from the LLMs.  Below is an example
     * on how to do this with a LLaMA2 LLM using a completions style API.
     *
     * HuggingFace style APIs will usually use the variable `done` below to
     * indicate the end of the stream.  OpenAI style APIs will often send a
     * string in the stream `"data: [DONE]\n"` to indicate the stream is complete.
     *
     * This example concatenates all values from the tokens into a single text
     * variable.  How you choose to use the tokens will likely be different, so
     * please modify the code.
     *
     * This examples assumes:
     * 1) You've followed the guide at
     * {@link https://docs.octoai.cloud/docs/how-to-create-an-octoai-access-token |
     * How to Create an OctoAI Access Token} to create and set your OctoAI access
     * token
     * 2) Either that you will set this token as an OCTOAI_TOKEN envvar
     * or edit the snippet to pass it as a value in the `{@link Client.constructor}`.
     * 3) You have assigned your endpoint URL and inputs into variables named
     * llamaEndpoint and streamInputs.
     *
     *```ts
     * const client = new Client();
     *     const readableStream = await client.inferStream(
     *       llamaEndpoint,
     *       streamInputs
     *     );
     * let text = ``;
     * const streamReader = readableStream.getReader();
     * for (
     *   let { value, done } = await streamReader.read();
     *   !done;
     *   { value, done } = await streamReader.read()
     * ) {
     *   if (done) break;
     *   const decoded = new TextDecoder().decode(value);
     *   if (
     *     decoded === "data: [DONE]\n" ||
     *     decoded.includes('"finish_reason": "')
     *   ) {
     *     break;
     *   }
     *   const token = JSON.parse(decoded.substring(5));
     *   if (token.object === "chat.completion.chunk") {
     *     text += token.choices[0].delta.content;
     *   }
     * console.log(text);
     *```
     * The `const token = JSON.parse(decoded.substring(5))` line strips `"data"`
     * from the returned text/event-stream then parses the token as an object.
     */
    async inferStream(endpointUrl, inputs) {
        const headers = { ...this.headers, Accept: "text/event-stream" };
        const response = await fetch(endpointUrl, {
            method: "POST",
            headers: headers,
            body: JSON.stringify(inputs),
        });
        if (!response.ok) {
            await (0, error_1.throwOctoAIError)(response);
        }
        return response;
    }
    coldStartWarning() {
        console.warn("Your endpoint may take several minutes to start " +
            "and be ready to serve inferences. You can increase your endpoint's " +
            "min replicas to mitigate cold starts.");
    }
    /**
     * Check health of an endpoint using a get request.  Try until timeout.
     *
     * @param endpointUrl - Target URL to run the health check.
     * @param timeoutMS - Milliseconds before request times out.  Default is 15
     * minutes.
     * @param intervalMS - Interval in milliseconds before the healthCheck method
     * queries
     * @returns HTTP status code.
     *
     * @remarks
     * The default timeout is set to 15 minutes to allow for potential cold start.
     *
     * For custom containers, please follow
     * {@link https://docs.octoai.cloud/docs/health-check-paths-in-custom-containers
     * | Health Check Paths in Custom Containers} to set a health check endpoint.
     *
     * Information about health check endpoint URLs are available on relevant
     * QuickStart Templates.
     */
    async healthCheck(endpointUrl, timeoutMS = 900000, // 15 minutes for cold start
    intervalMS = 1000) {
        const start = new Date().getTime();
        // Fence post
        let response = await fetch(endpointUrl, {
            method: "GET",
            headers: this.headers,
        });
        if (!response.ok) {
            this.coldStartWarning();
        }
        while (new Date().getTime() - start < timeoutMS && !response.ok) {
            if (400 <= response.status && response.status < 500) {
                await (0, error_1.throwOctoAIError)(response);
            }
            response = await fetch(endpointUrl, {
                method: "GET",
                headers: this.headers,
            });
            await new Promise((resolve) => setTimeout(resolve, intervalMS));
        }
        if (response.status >= 400) {
            await (0, error_1.throwOctoAIError)(response);
        }
        return response.status;
    }
    /**
     * Execute an inference in the background on the server.
     *
     * @param endpointUrl - Target URL to send inference request.
     * @param inputs - Contains necessary inputs for endpoint to run inference.
     * @returns Future allows checking if results are ready then accessing them.
     *
     * @remarks
     * Please read the {@link https://docs.octoai.cloud/reference/inference |
     * Async Inference Reference} for more information.
     * {@link Client.inferAsync} returns an {@link InferenceFuture},
     * which can then be used with {@link Client.isFutureReady} to see the
     * status.  Once it returns `true`, you can use the
     * {@link Client.getFutureResult} to get the response for your
     * InferenceFuture.
     *
     * Assuming you have a variable with your target endpoint URL and the inputs
     * the model needs, and an `OCTOAI_TOKEN` set as an environment variable, you
     * can run a server-side asynchronous inference from
     * {@link https://docs.octoai.cloud/docs/welcome-to-the-octoai-compute-service-copy | QuickStart Template}
     * endpoints with something like the below.
     *
     * ```ts
     *  const client = new Client();
     *  const future = await client.inferAsync(url, inputs);
     *  if (await client.isFutureReady(future) === true) {
     *    return await client.getFutureResult(future);
     *  }
     * ```
     */
    async inferAsync(endpointUrl, inputs) {
        const headers = { ...this.headers };
        headers["X-OctoAI-Async"] = "1"; // Enables server-side async
        const response = await fetch(endpointUrl, {
            method: "POST",
            headers: headers,
            body: JSON.stringify(inputs),
        });
        if (response.status >= 400) {
            await (0, error_1.throwOctoAIError)(response);
        }
        const future = (await response.json());
        return future;
    }
    async pollFuture(future) {
        const response = await fetch(future.poll_url, {
            method: "GET",
            headers: this.headers,
        });
        if (response.status >= 400) {
            await (0, error_1.throwOctoAIError)(response);
        }
        return (await response.json());
    }
    /**
     * Return whether the {@link InferenceFuture} generated from
     * {@link Client.inferAsync} has been computed and can return results.
     *
     * @param future - Created from {@link Client.inferAsync}.
     * @returns True if the {@link InferenceFuture}
     * inference is completed and are able to use {@link Client.getFutureResult}.
     * Else returns false.
     */
    async isFutureReady(future) {
        const respObject = await this.pollFuture(future);
        return "completed" === respObject.status;
    }
    /**
     * Return the result of a {@link InferenceFuture} generated from
     * {@link Client.inferAsync} as long as {@link Client.isFutureReady} returned
     * `true`.
     *
     * @param future - An {@link InferenceFuture} generated from
     * {@link Client.inferAsync}
     * @returns JSON outputs from the endpoint.
     */
    async getFutureResult(future) {
        const respObject = await this.pollFuture(future);
        if (respObject.status !== "completed") {
            await (0, error_1.throwOctoAIError)(new Response(null, {
                status: 425,
                statusText: "Wait until isFutureReady returns true before running getFutureResult.",
            }));
        }
        const responseUrl = respObject.response_url;
        const response = await fetch(responseUrl, {
            method: "GET",
            headers: this.headers,
        });
        if (response.status >= 400) {
            await (0, error_1.throwOctoAIError)(response);
        }
        return response.json();
    }
}
exports.Client = Client;
//# sourceMappingURL=data:application/json;base64,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